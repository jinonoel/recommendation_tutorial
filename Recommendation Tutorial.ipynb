{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.metrics.pairwise\n",
    "import math\n",
    "\n",
    "ratings = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "movies = pd.read_csv('./ml-latest-small/movies.csv')\n",
    "tags = pd.read_csv('./ml-latest-small/tags.csv')\n",
    "\n",
    "#Split 80%/20% between train and test\n",
    "train_ratings, test_ratings = sklearn.model_selection.train_test_split(ratings, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-based Filtering\n",
    "============\n",
    "Content-based filtering methods are based on a description of the item and a profile of the userâ€™s preferences\n",
    "\n",
    "Uses user and item features to create a recommendation model. Think of it as like any regression or classification problem in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Linear Regression\n",
    "------\n",
    "\n",
    "We'll use the movie genres as the movie features. Process it as one hot encoding vector for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process movie features\n",
    "genres = set()\n",
    "movie_genres = {}\n",
    "for index, row in movies.iterrows():\n",
    "    genres.add(row['genres'])\n",
    "    movie_genres[row['movieId']] = row['genres'].split(\"|\")\n",
    "    \n",
    "genres_a = []\n",
    "for g in genres:\n",
    "    genres_a.append(g)\n",
    "    \n",
    "movie_features = {}\n",
    "for key in movie_genres:\n",
    "    val = movie_genres[key]\n",
    "    movie_features[key] = []\n",
    "    for g in genres_a:\n",
    "        movie_features[key].append(1 if g in val else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "for index, row in train_ratings.iterrows():\n",
    "    train_features.append(movie_features[row['movieId']])\n",
    "    train_labels.append(row['rating'])\n",
    "    \n",
    "for index, row in test_ratings.iterrows():\n",
    "    test_features.append(movie_features[row['movieId']])\n",
    "    test_labels.append(row['rating'])\n",
    "    \n",
    "lr.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to evaluate our models. A common metric for evaluating recommendation algorithms is the Root Mean Squared Error (RMSE). \n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{N} \\sum\\limits_{i}^{N} (y_i-\\hat{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_ratings = lr.predict(test_features)\n",
    "\n",
    "rmse = math.sqrt(sklearn.metrics.mean_squared_error(predicted_ratings, test_labels))\n",
    "print \"RMSE:\", rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An un-regularized linear regression model with just using movie genres is getting an RMSE 1.038! That's not too bad already. You can probably even continue to try to improve this model by incorporating some user features (not available in movielens 100k) or adding regularization to the regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative Filtering\n",
    "============\n",
    "\n",
    "In collaborative filtering, we can make recommendations using only information about the past preferences of the the user along with other users. We don't need any user or item features, and it generally works better than content-based recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization:\n",
    "------\n",
    "<img src=\"./images/Collaborative_filtering-14.tiff\" style=\"height:300px;width:300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Collaborative_filtering-16.tiff\" style=\"height:300px;width:300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors\n",
    "--------------------\n",
    "\n",
    "A common form of collaborative filtering is the nearest neighbors method. The intuition behind it is:\n",
    "1. Get the similarities between users or items (More on this later)\n",
    "2. Weigh the recommendations of the nearest neighbors of the user based on the similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get lookup dict or ratings from movie to user perspective\n",
    "movie_users = {}\n",
    "user_movies = {}\n",
    "for index, row in train_ratings.iterrows():\n",
    "    if row['movieId'] not in movie_users:\n",
    "        movie_users[row['movieId']] = {}\n",
    "        \n",
    "    if row['userId'] not in user_movies:\n",
    "        user_movies[row['userId']] = {}\n",
    "        \n",
    "    movie_users[row['movieId']][row['userId']] = row['rating']\n",
    "    user_movies[row['userId']][row['movieId']] = row['rating']\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a similarity metric. A simple one we can use is the cosine similarity between two vectors A and B. In this setting A and B are the ratings each user has given to movies that they have <b>both watched</b>.\n",
    "\n",
    "$$\\cos(A,B)=\\frac{\\sum\\limits_{i}^{N}A_iB_i}{\\sqrt{\\sum\\limits_{i}^{N}A_i^2} \\sqrt{\\sum\\limits_{i}^{N}B_i^2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculate similarity ratings for all user combinations\n",
    "#Uses cosine similarity\n",
    "similarities = {}\n",
    "\n",
    "for user1 in user_movies:\n",
    "    if user1 not in similarities:\n",
    "        similarities[user1] = {}\n",
    "    \n",
    "    for user2 in user_movies:\n",
    "        if user2 in similarities[user1] or user1 == user2:\n",
    "            continue\n",
    "        \n",
    "        user1_ratings = []\n",
    "        user2_ratings = []\n",
    "        \n",
    "        for key in user_movies[user1]:\n",
    "            watched_users = movie_users[key]\n",
    "            if user1 in watched_users and user2 in watched_users:\n",
    "                user1_ratings.append(watched_users[user1])\n",
    "                user2_ratings.append(watched_users[user2])\n",
    "                \n",
    "        if len(user1_ratings) > 0:\n",
    "            if user2 not in similarities:\n",
    "                similarities[user2] = {}\n",
    "            \n",
    "            sim = np.dot(user1_ratings, user2_ratings) / (np.linalg.norm(user1_ratings) * np.linalg.norm(user2_ratings))\n",
    "            similarities[user1][user2] = sim\n",
    "            similarities[user2][user1] = sim\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the similarities, we can use nearest neighbor regression using the following formula:\n",
    "\n",
    "$$R_{x,y} = \\frac{\\sum\\limits_{z} Sim_{x,z}R_{z,y}}{\\sum\\limits_{z} Sim_{x,z}}$$\n",
    "\n",
    "Basically you weigh the neighbor's rating by their similarity to the target user, sum them up, and normalize by the sum of the similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nearest neighbor regression\n",
    "def knn_predict(k, userId, movieId):\n",
    "    watched_users = movie_users[movieId]\n",
    "    \n",
    "    sim_scores = []\n",
    "    rating_scores = []\n",
    "    \n",
    "    for w in watched_users:\n",
    "        if w in similarities[userId]:\n",
    "            sim_scores.append(similarities[userId][w])\n",
    "            rating_scores.append(watched_users[w])\n",
    "    \n",
    "    #Got lazy. Used pandas DataFrames to sort by similarity and get the nearest neighbors\n",
    "    if len(sim_scores) > 0:\n",
    "        sim_df = pd.DataFrame(data={'sim' : sim_scores, 'rating' : rating_scores})\n",
    "        sorted_sim_df = sim_df.sort_values(by=['sim'], ascending = False)\n",
    "        knearest = sorted_sim_df.head(k)\n",
    "        \n",
    "        numerator = 0\n",
    "        denomenator = 0\n",
    "        for index, row in knearest.iterrows():\n",
    "            #print row['sim'], row['rating']\n",
    "            numerator += row['sim'] * row['rating']\n",
    "            denomenator += row['sim']\n",
    "        #print \"Quotient\", numerator, denomenator, numerator / denomenator  \n",
    "        return numerator / denomenator\n",
    "    else: #What to do if user has no similarities??\n",
    "        return 2.5\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = []\n",
    "actual_ratings = []\n",
    "\n",
    "K=25\n",
    "for index, row in test_ratings.iterrows():\n",
    "    if row['movieId'] not in movie_users:\n",
    "        continue\n",
    "        \n",
    "    pred = knn_predict(K, row['userId'], row['movieId'])\n",
    "    predicted_ratings.append(pred)\n",
    "    actual_ratings.append(row['rating'])\n",
    "    \n",
    "rmse = math.sqrt(sklearn.metrics.mean_squared_error(predicted_ratings, actual_ratings))\n",
    "print \"RMSE:\", rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<table>\n",
    "        <th>K</th>\n",
    "        <th>RMSE</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td>1.03728861441</td>\n",
    "    <tr>\n",
    "        <td>10</td>\n",
    "        <td>1.00264603453</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>15</td>\n",
    "        <td>0.992588621898</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>20</td> \n",
    "        <td>0.989465179239</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>25</td> \n",
    "        <td>0.988360543936</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Two main things you can fiddle with here aside from K:\n",
    "\n",
    "1. Use item similarity instead of user. This was pioneered by Amazon in 1998 and documented in papers they published in 2001 and 2003 (https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf). This works especially better when there are fewer items than users, so similarities will be richer due to more intersections.\n",
    "\n",
    "2. Try different similarity metrics. For example: Pearson correlation which goes from -1 to 1 instead of just 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Factorization\n",
    "--------\n",
    "\n",
    "A more powerful form of collaborative filtering is matrix factorization. The basic intuition is that it models the user-item ratings/preferences as an $M \\times N$ matrix $R$ where $M$ is the number of users and $N$ is the number of items.\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>Movie1</th>\n",
    "        <th>Movie2</th>\n",
    "        <th>Movie3</th>\n",
    "        <th>Movie4</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>User1</th>\n",
    "        <td>5</td>\n",
    "        <td></td>\n",
    "        <td>1.5</td>\n",
    "        <td>3.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>User2</th>\n",
    "        <td>1</td>\n",
    "        <td>3.5</td>\n",
    "        <td></td>\n",
    "        <td>4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>User3</th>\n",
    "        <td></td>\n",
    "        <td>2</td>\n",
    "        <td>4</td>\n",
    "        <td>5</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "There are different kinds of matrix factorization methods for CF. A simple one is Probabilistic Matrix Factorization (https://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf). In PMF the ratings matrix $R$ is decomposed into an $M \\times K$ matrix $U$ and a $N \\times K$ matrix $V$ such that\n",
    "\n",
    "$$ R = U^TV$$\n",
    "\n",
    "### Learning $U$ and $V$ ######\n",
    "\n",
    "So how do we learn U and V? We can do gradient descent using Alternating Least Squares to minimize the following error function:\n",
    "\n",
    "$$ \n",
    "E = \\frac{1}{2} \\sum\\limits_{i}^{N}\\sum\\limits_{j}^{M}I_{i,j}(R_{i,j} - U_{i}^{T}V_{j})^2 \n",
    "+ \\frac{\\lambda_U}{2} \\sum\\limits_{i}^{N} \\|{U_i}\\|_{Fro}^2\n",
    "+ \\frac{\\lambda_V}{2} \\sum\\limits_{j}^{M} \\|{V_j}\\|_{Fro}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(user_matrix, item_matrix, actual, lu, li):\n",
    "    error = 0\n",
    "    for u in actual:\n",
    "        for i in actual[u]:\n",
    "            pred = np.dot(user_matrix[u], item_matrix[i])\n",
    "            error += (pred - actual[u][i]) ** 2\n",
    "            \n",
    "    error /= 2\n",
    "    \n",
    "    u_norm = 0\n",
    "    for u in user_matrix:\n",
    "        for val in user_matrix[u]:\n",
    "            u_norm += val ** 2\n",
    "    i_norm = 0\n",
    "    for i in item_matrix:\n",
    "        for val in item_matrix[i]:\n",
    "            i_norm += val ** 2\n",
    "            \n",
    "    error += lu * u_norm / 2\n",
    "    error += li * i_norm / 2\n",
    "              \n",
    "    return error\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derivative with respect to U\n",
    "def dE_u(user_matrix, item_matrix, actual, lu, u, index):\n",
    "    derivative = 0\n",
    "    for i in actual[u]:\n",
    "        pred = np.dot(user_matrix[u], item_matrix[i])\n",
    "        derivative += (pred - actual[u][i]) * item_matrix[i][index]\n",
    "        \n",
    "    derivative += lu * user_matrix[u][index]\n",
    "    \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derivative with respect to V\n",
    "def dE_i(user_matrix, item_matrix, actual, li, i, index):\n",
    "    derivative = 0\n",
    "    watched_users = movie_users[i]\n",
    "    for u in watched_users:\n",
    "        pred = np.dot(user_matrix[u], item_matrix[i])\n",
    "        derivative += (pred - actual[u][i]) * user_matrix[u][index]\n",
    "        \n",
    "    derivative += li * item_matrix[i][index]\n",
    "    \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Alternating Least Squares, we get the gradients of $E$ by fixing $V$ as constants and getting the derivative of $E$ with respect to $U$ and vice versa. We alternate between both gradients when we update the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternating Least Squares\n",
    "def learn(user_matrix, item_matrix, actual, lu, li, learning_rate):\n",
    "    counter = 0\n",
    "    while counter < 1000:\n",
    "        error = get_error(user_matrix, item_matrix, actual, lu, li)\n",
    "        print \"Counter:\", counter, \"Error:\", error #Error should trend downwards over time\n",
    "        \n",
    "        #Treat V as contant, update U with its gradients\n",
    "        if counter % 2 == 0:\n",
    "            user_derivatives = {}      \n",
    "            for u in user_matrix:\n",
    "                user_derivatives[u] = []\n",
    "            \n",
    "                for index in range(len(user_matrix[u])):\n",
    "                    user_derivatives[u].append(dE_u(user_matrix, item_matrix, actual, lu, u, index))\n",
    "                    \n",
    "            for u in user_matrix:\n",
    "                for index in range(len(user_matrix[u])):\n",
    "                    user_matrix[u][index] -= learning_rate * user_derivatives[u][index]\n",
    "\n",
    "        #Treat U as constant, update V with its gradients\n",
    "        else:\n",
    "            item_derivatives = {}\n",
    "            for i in item_matrix:\n",
    "                item_derivatives[i] = []\n",
    "                for index in range(len(item_matrix[i])):\n",
    "                    item_derivatives[i].append(dE_i(user_matrix, item_matrix, actual, li, i, index))\n",
    "                    \n",
    "            for i in item_matrix:\n",
    "                for index in range(len(item_matrix[i])):\n",
    "                    item_matrix[i][index] -= learning_rate * item_derivatives[i][index]\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "    return (user_matrix, item_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_matrix = {}\n",
    "item_matrix = {}\n",
    "learning_rate = 0.0001\n",
    "lu = 0.0001\n",
    "li = 0.0001\n",
    "K = 5\n",
    "\n",
    "for u in user_movies:\n",
    "    user_matrix[u] = random.sample(range(1, 100), K)\n",
    "    for index in range(K):\n",
    "        user_matrix[u][index] /= float(100)\n",
    "    \n",
    "for i in movie_users:\n",
    "    item_matrix[i] = random.sample(range(1, 100), K)\n",
    "    for index in range(K):\n",
    "        item_matrix[i][index] /= float(100)\n",
    "    \n",
    "learned_users, learned_items = learn(user_matrix, item_matrix, user_movies, lu, li, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = []\n",
    "actuals = []\n",
    "not_count = 0\n",
    "for index, row in test_ratings.iterrows():\n",
    "    if row['movieId'] not in movie_users:\n",
    "        continue\n",
    "        \n",
    "    pred = np.dot(learned_users[row['userId']], learned_items[row['movieId']])\n",
    "    predicted_ratings.append(pred)\n",
    "    actuals.append(row['rating'])\n",
    "\n",
    "rmse = math.sqrt(sklearn.metrics.mean_squared_error(predicted_ratings, actuals))\n",
    "print \"RMSE:\", rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th colspan=2><center>K=5</center></th>\n",
    "    <tr>\n",
    "        <th>$\\lambda$</th>\n",
    "        <th>RMSE</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.0001</td>\n",
    "        <td>0.858861345016</td>\n",
    "    <tr>\n",
    "        <td>0.001</td>\n",
    "        <td>0.913146934761</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.01</td>\n",
    "        <td>0.912297969849</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.1</td> \n",
    "        <td>0.913277052092</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>0.916509506566</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>10</td>\n",
    "        <td>1.01069785051</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th colspan=2><center>K=10</center></th>\n",
    "    <tr>\n",
    "        <th>$\\lambda$</th>\n",
    "        <th>RMSE</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.0001</td>\n",
    "        <td>0.91227081992</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.001</td>\n",
    "        <td>0.908526077502</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.01</td>\n",
    "        <td>0.910726843867</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.1</td> \n",
    "        <td>0.846665725031</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>0.848858193242</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>10</td>\n",
    "        <td>0.927015692586</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "==========\n",
    "\n",
    "1. Various methods have been devised to combine collaborative filtering and content-based recommendations into hybrid recommender systems (For example: https://www.microsoft.com/en-us/research/wp-content/uploads/2009/01/www09.pdf). \n",
    "\n",
    "\n",
    "2. Use frameworks :) Faster and easier. http://surpriselib.com\n",
    "\n",
    "\n",
    "3. Session-based click stream can be used instead of past user preferences, using RNNs (https://arxiv.org/pdf/1511.06939.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
